?seq
a = seq(0,1,length.out=100)
a
a = seq(0,1,length.out=100)
l = 1
plot(a,a*(1-a)^l)
l = 2
plot(a,a*(1-a)^l)
l = 3
plot(a,a*(1-a)^l)
l = 4
plot(a,a*(1-a)^l)
l = 5
plot(a,a*(1-a)^l)
plot(a,a*(1-a)^l,ylim=c(0,0.25))
par(mfrow=c(2,2))
plot(a,a*(1-a)^l,ylim=c(0,0.25))
for (l in 1:4){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
}
par(mfrow=c(2,2))
for (l in 1:4){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
}
for (l in 5:8){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
}
par(mfrow=c(2,2))
for (l in 5:8){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
title(paste0('l = 'l))
}
par(mfrow=c(2,2))
for (l in 5:8){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
title(paste0('l = ',l))
}
par(mfrow=c(2,2))
for (l in 1:4){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
title(paste0('lag = ',l))
}
par(mfrow=c(2,3))
for (l in 1:6){
plot(a,a*(1-a)^l,ylim=c(0,0.25))
title(paste0('lag = ',l))
}
1.09^12
1.0931^12
2.9^(1/12)
1.05^12
1.8^(1/12)
a = seq(0,1,length.out = 100)
a
a = seq(0,1,length.out = 100)
par(mfrow=c(2,2))
for (i in 1:4){
plot(a,a*(1-a)^i);title(paste0('lag = ',i))
}
plot(a,a*(1-a)^i,ylim=c(0,0.25));title(paste0('lag = ',i))
a = seq(0,1,length.out = 100)
par(mfrow=c(2,2))
for (i in 1:4){
plot(a,a*(1-a)^i,ylim=c(0,0.25));title(paste0('lag = ',i))
}
a = seq(0,1,length.out = 1000)
par(mfrow=c(2,2))
for (i in 1:4){
plot(a,a*(1-a)^i,ylim=c(0,0.25));title(paste0('lag = ',i))
}
a = seq(0,1,length.out = 200)
par(mfrow=c(2,2))
for (i in 1:4){
plot(a,a*(1-a)^i,ylim=c(0,0.25));title(paste0('lag = ',i))
}
knn
?knn
??knn
method(predict()
method(predict)
methods(predict)
data(iris)
iris
## test set 만들기 ##
data(iris)
iris
idx = sample(1:nrow(iris),nrow(iris)*0.8)
idx
head(iris)
X_train = iris[idx,1:4]
X_train
y_train = iris$Species
y_train
y_test = iris$Species[-idx]
y_test
str(X_train)
distances = rep(0,nrow(X_train))
distances
__a = 1
x = x_test[1,]
x
X_test = X_test
X_test = iris[-idx,1:4]
idx = sample(1:nrow(iris),nrow(iris)*0.8)
X_train = iris[idx,1:4]
X_test = iris[-idx,1:4]
y_train = iris$Species[idx]
y_test = iris$Species[-idx]
k = 3
X_train = X_train
y_train = y_train
X_test = X_test
x = X_test[1,]
i-1
i=1
euclidean_distance(x,X_train[i])
## 모델구성 ##
euclidean_distance = function(x1,x2){
return(sqrt(sum((x1-x2)^2)))
}
euclidean_distance(x,X_train[i])
x
x_train[i]
X_train[i]
euclidean_distance(x,X_train[i,])
x-X_train[i]
x-X_train[i,]
(x-X_train[i,])^2
sum((x-X_train[i,])^2)
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
distances
rank(distances)
order(distances)
distances
order(distances)
a = c(1, 4, 2 ,1, 6, 1, 3 ,5 ,8)
sort(a)
order(a)
k_indices = order(distances)[1:k]
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
k_indices =
data(iris)
idx = sample(1:nrow(iris),nrow(iris)*0.8)
X_train = iris[idx,1:4]
X_test = iris[-idx,1:4]
y_train = iris$Species[idx]
y_test = iris$Species[-idx]
euclidean_distance = function(x1,x2){
return(sqrt(sum((x1-x2)^2)))
}
k = 3
X_train = X_train
y_train = y_train
X_test = X_test
x = X_test[1,]
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
k_indices = order(distances)[1:k]
k_indices
y_train[k_indices]
k_nearest_labels = y_train[k_indices]
table(k_nearest_labels)
table(k_nearest_labels)[1]
table(k_nearest_labels)[1]
names(table(k_nearest_labels)[1])
a = c(1,2,2,3)
table(a)
k_nearest_labels
which.max(table(a))
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
most_common
k_nearest_labels
table(k_nearest_labels)
which.max(table(k_nearest_labels))
max(table(k_nearest_labels))
which.max(table(k_nearest_labels))
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
names(most_common)
predict(x)
predict = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
predict(x)
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
predict_(x)
predicted_labels = rep(0,NROW(X_test))
i=1
X_test[i,]
for (1:NROW(X_test)){
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
x = X_test[i,]
predicted_labels[i] = predict_(x)
}
for (i = 1:NROW(X_test)){
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
x = X_test[i,]
predicted_labels[i] = predict_(x)
}
for (i in 1:NROW(X_test)){
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
x = X_test[i,]
predicted_labels[i] = predict_(x)
}
predicted_labels
for (i in 1:NROW(X_test)){
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
x = X_test[i,]
predicted_labels[i] = predict_(x)
}
apply(X_test,1,predict_)
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
predicted_labels = apply(X_test,1,predict_)
## 함수 적용해보기 ##
y_pred = KNN(k=3, X_train, y_train,X_test)
KNN = function(k=3, X_train, y_train, X_test){
# k = 3
# X_train = X_train
# y_train = y_train
# X_test = X_test
predict_ = function(x){
# compute distances
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
# get k nearest samples, labels
k_indices = order(distances)[1:k]
k_nearest_labels = y_train[k_indices]
# majority vote, most common class label
most_common = which.max(table(k_nearest_labels))
return(names(most_common))
}
predicted_labels = apply(X_test,1,predict_)
return(predicted_labels)
}
## 함수 적용해보기 ##
y_pred = KNN(k=3, X_train, y_train,X_test)
y_test==y_pred
acc = mean(y_test==y_pred)
acc
""" a """
'''
a
'''
'''
a
'''
python과의비교 = "
"
setwd("G:/내 드라이브/gdrive/work/model_code_from_scratch")
euclidean_distance(1:10,11:20)
euclidean_distance = function(a,b){
# we check that they have the same number of observation
if (length(a) == length(b)){
sqrt(sum((a-b)^2))
} else {
stop('Vectors must be of the same length')
}
}
euclidean_distance(1:10,11:20)
euclidean_distance = function(a,b){
# we check that they have the same number of observation
if (length(a) == length(b)){
sqrt(sum((a-b)^2))
} else {
stop('Vectors must be of the same length')
}
}
euclidean_distance(1:10,11:20)
euclidean_distance(1:10,11:19)
## 4. 배운거 정리 & 느낀점 ##
python과의비교 <-
"
np.sort : sort
np.argsort : order
파이썬에서는 list comprehension을 활용한 for문이 상당히 많다.
이걸 R에서 다 구현하면 너무 느림....
파이썬으로 짠 knn을 최대한 그대로 번역하는 느낌으로 해보려고 하였으며
for문 하나는 apply로 대체. 유클리드 거리구하는것도 apply로 바꿔야 더 빠를듯.150개 밖에 안되는거도 1~2초 걸리는걸로봐서 데이터 커지면 답도없음
파이썬에서 class 형태로 코딩하는걸 어떻게 R에서 구현해야할지 잘모르겠음. 앞으로 하다보면 감이 오겠지.
"
library(shiny)
install.packages('shiny')
library(shiny)
runGitHub('Shiny','insooAI',ref='main')
install.packages('rpart.plot')
runGitHub('Shiny','insooAI',ref='main')
install.packages('rattle')
runGitHub('Shiny','insooAI',ref='main')
install.packages('onewaytests')
runGitHub('Shiny','insooAI',ref='main')
install.packages('gganimate')
runGitHub('Shiny','insooAI',ref='main')
install.packages('tidymodels')
install.packages("tidymodels")
runGitHub('Shiny','insooAI',ref='main')
install.packages('broom')
install.packages("broom")
runGitHub('Shiny','insooAI',ref='main')
