y_test
str(X_train)
distances = rep(0,nrow(X_train))
distances
__a = 1
x = x_test[1,]
x
X_test = X_test
X_test = iris[-idx,1:4]
idx = sample(1:nrow(iris),nrow(iris)*0.8)
X_train = iris[idx,1:4]
X_test = iris[-idx,1:4]
y_train = iris$Species[idx]
y_test = iris$Species[-idx]
k = 3
X_train = X_train
y_train = y_train
X_test = X_test
x = X_test[1,]
i-1
i=1
euclidean_distance(x,X_train[i])
## 모델구성 ##
euclidean_distance = function(x1,x2){
return(sqrt(sum((x1-x2)^2)))
}
euclidean_distance(x,X_train[i])
x
x_train[i]
X_train[i]
euclidean_distance(x,X_train[i,])
x-X_train[i]
x-X_train[i,]
(x-X_train[i,])^2
sum((x-X_train[i,])^2)
distances = rep(0,nrow(X_train))
for (i in 1:nrow(X_train)){
distances[i] = euclidean_distance(x,X_train[i,])
}
distances
rank(distances)
order(distances)
distances
order(distances)
library(shiny)
runGitHub('Shiny','insooAI',ref='main')
install.packages('ranger')
runGitHub('Shiny','insooAI',ref='main')
install.packages('vip')
runGitHub('Shiny','insooAI',ref='main')
install.packages('randomForest')
runGitHub('Shiny','insooAI',ref='main')
install.packages('forecast')
runGitHub('Shiny','insooAI',ref='main')
install.packages('plotly')
runGitHub('Shiny','insooAI',ref='main')
install.packages('ggthemes')
runGitHub('Shiny','insooAI',ref='main')
setwd("G:/내 드라이브/gdrive/work/model_code_from_scratch")
?mutate
??mutate
N <- 200 # number of points per class
D <- 2 # dimensionality, we use 2D data for easy visulization
K <- 2 # number of classes, binary for logistic regression
X <- data.frame() # data matrix (each row = single example, can view as xy coordinates)
y <- data.frame() # class labels
set.seed(56)
for (j in (1:K)){
# t, m are parameters of parametric equations x1, x2
t <- seq(0,1,length.out = N)
# add randomness
m <- rnorm(N, j+0.5, 0.25)
Xtemp <- data.frame(x1 = 3*t , x2 = m - t)
ytemp <- data.frame(matrix(j-1, N, 1))
X <- rbind(X, Xtemp)
y <- rbind(y, ytemp)
}
X
cbind(1,X)
cbind(bias=1,X)
head(cbind(bias=1,X))
head(cbind('bias'=1,X))
y
as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),nrow(X))
theta
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
theta
X
typeof(X)
X
as.data.frame(X)
is.data.frame(X)
cbind('bias'=1,X)
as.matrix(cbind('bias'=1,X))
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
t
plot(t)
plot(m)
matrix(j-1,N,1)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
y
N = 200
D = 2   # dimension
K = 2   # number of classes
X = data.frame()
y = data.frame()
set.seed(56)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
nrow(X)
y
data = cbind(X,y)
colnames(data) = c(colnames(X), 'label')
head(data)
data = cbind(y,X)
colnames(data) = c('label',colnames(X))
head(data)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)
library(ggplot2)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitie('Data to be classified')+
theme_bw()
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw()
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)+
theme(legend.positionc(0.85,0.87))
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)+
theme(legend.position(0.85,0.87))
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)+
theme(legend.position=c(0.85,0.87))
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
#### 3. Logistic Regression ####
#### Language : R ####
# source : https://towardsdatascience.com/logistic-regression-from-scratch-in-r-b5b122fd8e83
## 1. 모델구성 ##
## sigmoid function, inverse of logit
sigmoid <- function(z) 1 / (1 + exp(-z))
## cost function
cost <- function(theta, X, y){
m = length(y)
h = sigmiod(X %*% theta)
J = (t(-y) %*% log(h) - t(1-y) %*% log(1-h)) / m
return(J)
}
## gradient function
grad <- function(theta, X, y){
m = length(y)
h = sigmoid(X %*% theta)
grad = (t(X) %*% (h - y)) / m
return(grad)
}
logisticReg <- function(X, y){
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = cbind('bias'=1,X)
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = x, y = y)
return(costOpti$par)
}
## probability of getting 1
logisticProb <- function(theta, X){
X = na.omit(X)
# add bias term
X = as.matrix(cbind('bias'=1,X))
return(sigmoid(X %*% theta))
}
## y prediction
logisticPred <- function(prob){
return(round(prob,0))
}
## 2. sample data 만들기
N = 200
D = 2   # dimension
K = 2   # number of classes
X = data.frame()
y = data.frame()
set.seed(56)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
data = cbind(y,X)
colnames(data) = c('label',colnames(X))
library(ggplot2)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)+
theme(legend.position=c(0.85,0.87))
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## cost function
cost <- function(theta, X, y){
m = length(y)
h = sigmoid(X %*% theta)
J = (t(-y) %*% log(h) - t(1-y) %*% log(1-h)) / m
return(J)
}
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
#### 3. Logistic Regression ####
#### Language : R ####
# source : https://towardsdatascience.com/logistic-regression-from-scratch-in-r-b5b122fd8e83
## 1. 모델구성 ##
## sigmoid function, inverse of logit
sigmoid <- function(z) 1 / (1 + exp(-z))
## cost function
cost <- function(theta, X, y){
m = length(y)
h = sigmoid(X %*% theta)
J = (t(-y) %*% log(h) - t(1-y) %*% log(1-h)) / m
return(J)
}
## gradient function
grad <- function(theta, X, y){
m = length(y)
h = sigmoid(X %*% theta)
grad = (t(X) %*% (h - y)) / m
return(grad)
}
logisticReg <- function(X, y){
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = cbind('bias'=1,X)
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = x, y = y)
return(costOpti$par)
}
## probability of getting 1
logisticProb <- function(theta, X){
X = na.omit(X)
# add bias term
X = as.matrix(cbind('bias'=1,X))
return(sigmoid(X %*% theta))
}
## y prediction
logisticPred <- function(prob){
return(round(prob,0))
}
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
logisticReg <- function(X, y){
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = cbind('bias'=1,X)
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = X, y = y)
return(costOpti$par)
}
## probability of getting 1
logisticProb <- function(theta, X){
X = na.omit(X)
# add bias term
X = as.matrix(cbind('bias'=1,X))
return(sigmoid(X %*% theta))
}
## y prediction
logisticPred <- function(prob){
return(round(prob,0))
}
## 2. sample data 만들기
N = 200
D = 2   # dimension
K = 2   # number of classes
X = data.frame()
y = data.frame()
set.seed(56)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
data = cbind(y,X)
colnames(data) = c('label',colnames(X))
library(ggplot2)
ggplot(data)+
geom_point(aes(x=x1, y=x2, color=as.character(label)),size=2)+
scale_colour_discrete(name='Label')+
ylim(0,3)+coord_fixed(ratio=1)+
ggtitle('Data to be classified')+
theme_bw(base_size=12)+
theme(legend.position=c(0.85,0.87))
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
logisticReg <- function(X, y){
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = cbind('bias'=1,X)
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = X, y = y)
return(costOpti$par)
}
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
logisticReg(X, y)
# remove NA rows
X = na.omit(X)
y = na.omit(y)
X
# add bias term and convert to  matrix
X = cbind('bias'=1,X)
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
theta
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = X, y = y)
X
X %*% theta
dim(X)
dim(theta)
logisticReg <- function(X, y){
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = as.matrix(cbind('bias'=1,X))
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = X, y = y)
return(costOpti$par)
}
# remove NA rows
X = na.omit(X)
y = na.omit(y)
# add bias term and convert to  matrix
X = as.matrix(cbind('bias'=1,X))
y = as.matrix(y)
# initialize theta
theta = matrix(rep(0,ncol(X)),ncol(X))
# use the optim function to perform gradient descent
costOpti = optim(theta, fn = cost, gr = grad, X = X, y = y)
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
theta
# remove NA rows
X = na.omit(X)
N = 200
D = 2   # dimension
K = 2   # number of classes
X = data.frame()
y = data.frame()
set.seed(56)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
## 3. 함수 적용해보기
## training
theta = logisticReg(X, y)
theat
theta
grid
## generate a grid for decision boundary, this is the test set
grid = expand.grid(seq(0, 3, length.out=100), seq(0, 3, length.out=100))
grid
## generate a grid for decision boundary, this is the test set
grid = expand.grid(seq(0, 3, length.out=100), seq(0, 3, length.out=100))
probZ = logisticProb(theta, grid)
Z = logisticPred(probZ)
gridPred = cbind(grid, Z)
head(Z)
head(gridPred)
ggplot() +   geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) +
geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F)+
ggtitle('Decision Boundary for Logistic Regression') +
coord_fixed(ratio = 1) +
theme_bw(base_size = 12)
ggplot() +   geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F)
ggplot() +   geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) +
geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F)
N = 200
D = 2   # dimension
K = 2   # number of classes
X = data.frame()
y = data.frame()
set.seed(56)
for (j in 1:K){
# t,m are parameters of parametric equations x1,x2
t = seq(0,1,length.out = N)
m = rnorm(N, j+0.5, 0.25)
Xtemp = data.frame(x1 = 3*t, x2 = m - t)
ytemp = data.frame(matrix(j-1,N,1))
X = rbind(X, Xtemp)
y = rbind(y, ytemp)
}
